{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rotate setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict by image align (before ptmap)\n",
    "ROTATE = 270 # 逆时针\n",
    "img_align_result = f\"/data/home/yanghanlong/results/image_align/ptmap_5k/my_model/rotate/rotate_{ROTATE}/v3_5_8_epoch9\"\n",
    "# img_align_result = \"/data/home/yanghanlong/results/image_align/my_model/ptmap_5k/my_model/v2_2_epoch3_remove_isopt_4\"\n",
    "# img_align_result = \"/data/home/yanghanlong/results/image_align/original_model/ptmap_5k_with_rtps_params\"\n",
    "\n",
    "## output dir after ptmap\n",
    "output_after_ptmap_dir = f\"/data/home/yanghanlong/results/pt_map/eval_mymodel_image_align/rotate_{ROTATE}/v3_5_8_epoch9\"\n",
    "# output_after_ptmap_dir = \"/data/home/yanghanlong/results/pt_map/eval_mymodel_image_align/original_model/ptmap5k\"\n",
    "\n",
    "## file path of template pts coord from <data/image_registration/anno/imageid.json>\n",
    "tem_pts_coord_dir = \"/data/home/yanghanlong/data/image_registration/anno\"\n",
    "\n",
    "## image path to get w, h\n",
    "image_dir = f\"/data/home/yanghanlong/data/image_retrieval/ptmap_5k_rotate/{ROTATE}\"\n",
    "\n",
    "expanded = False # no use; maybe discarded\n",
    "## make dir\n",
    "Path(f\"{output_after_ptmap_dir}/results\").mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normal setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict by image align (before ptmap)\n",
    "# ROTATE = 0 # 逆时针\n",
    "# img_align_result = \"/data/home/yanghanlong/results/image_align/ptmap_5k/my_model/rotate/rotate_0/v3_5_8_epoch9\"\n",
    "# # img_align_result = \"/data/home/yanghanlong/results/image_align/ptmap_5k/my_model/v0811_delete_selfmatchpt\" # 线上\n",
    "# # img_align_result = \"/data/home/yanghanlong/results/image_align/original_model/ptmap_5k_with_rtps_params\"\n",
    "\n",
    "# ## output dir after ptmap\n",
    "# output_after_ptmap_dir = \"/data/home/yanghanlong/results/pt_map/eval_mymodel_image_align/v3_5_8_epoch9\"\n",
    "# # output_after_ptmap_dir = \"/data/home/yanghanlong/results/pt_map/eval_mymodel_image_align/original_model/ptmap5k\"\n",
    "\n",
    "# ## file path of template pts coord from <data/image_registration/anno/imageid.json>\n",
    "# tem_pts_coord_dir = \"/data/home/yanghanlong/data/image_registration/anno\"\n",
    "\n",
    "# ## image path to get w, h\n",
    "# image_dir = f\"/data/home/yanghanlong/data/image_retrieval/image\"\n",
    "\n",
    "# expanded = False\n",
    "# ## make dir\n",
    "# Path(f\"{output_after_ptmap_dir}/results\").mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTMAP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_template_pt(template_pt, H, tps_weights, template_mkpts_tps_,\n",
    "                            parallex, overlap, K_smooth = 5):\n",
    "    '''\n",
    "    Args:\n",
    "        template_pt: ndarray, (2, ),\n",
    "        template_mkpts_tps_: ndarray, (2, n),\n",
    "        parallex: ndarray, (2, n)\n",
    "    '''\n",
    "    ## Do global homographic transform\n",
    "    template_hpt = np.append(template_pt, 1) # (3, )\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    template_hpt_ = H @ template_hpt # (3, )\n",
    "    template_pt_ = template_hpt_ / (template_hpt_[2] + 1e-8)\n",
    "    u_, v_ = template_pt_[:2]\n",
    "\n",
    "    ## Do tps transformation\n",
    "    gx, hy = 0, 0\n",
    "    offset_u0_, offset_u1_ = overlap['offset_u0_'], overlap['offset_u1_']\n",
    "    offset_v0_, offset_v1_ = overlap['offset_v0_'], overlap['offset_v1_']\n",
    "    if (offset_u0_ <= u_ <= offset_u1_) and (offset_v0_ <= v_ <= offset_v1_):\n",
    "        x1_, y1_ = template_mkpts_tps_\n",
    "        n = len(x1_)\n",
    "        gx_sub, hy_sub = 0, 0\n",
    "        wx, wy = tps_weights['wx'], tps_weights['wy']\n",
    "        a, b = tps_weights['a'], tps_weights['b']\n",
    "        for kf in range(n):\n",
    "            dist2 = (u_ - x1_[kf]) ** 2 + (v_ - y1_[kf]) ** 2\n",
    "            rbf = 0.5 * dist2 * np.log(dist2)\n",
    "            gx_sub += wx[kf] * rbf\n",
    "            hy_sub += wy[kf] * rbf\n",
    "\n",
    "        gx_sub += a[0] * u_ + a[1] * v_ + a[2]\n",
    "        hy_sub += b[0] * u_ + b[1] * v_ + b[2]\n",
    "        gx, hy = gx_sub, hy_sub\n",
    "\n",
    "    ## smooth transition to global transform\n",
    "    gxn, hyn = parallex\n",
    "    eta_d0 = 0\n",
    "    eta_d1 = K_smooth * max(abs(np.concatenate([gxn, hyn])))\n",
    "    sub_u0_, sub_u1_ = overlap['sub_u0_'], overlap['sub_u1_']\n",
    "    sub_v0_, sub_v1_ = overlap['sub_v0_'], overlap['sub_v1_']\n",
    "    sub_u0_ = sub_u0_ + min(gxn)\n",
    "    sub_u1_ = sub_u1_ + max(gxn)\n",
    "    sub_v0_ = sub_v0_ + min(hyn)\n",
    "    sub_v1_ = sub_v1_ + max(hyn)\n",
    "    dist_horizontal = np.maximum(sub_u0_-u_, u_-sub_u1_)\n",
    "    dist_vertical = np.maximum(sub_v0_-v_, v_-sub_v1_)\n",
    "    dist_sub = np.maximum(dist_horizontal, dist_vertical)\n",
    "    dist_sub = np.maximum(0, dist_sub)\n",
    "    eta = (eta_d1 - dist_sub) / (eta_d1 - eta_d0)\n",
    "    # eta[dist_sub < eta_d0] = 1\n",
    "    # eta[dist_sub > eta_d1] = 0\n",
    "    if dist_sub < eta_d0:\n",
    "        eta = 1\n",
    "    elif dist_sub > eta_d1:\n",
    "        eta = 0\n",
    "\n",
    "    gx *= eta\n",
    "    hy *= eta\n",
    "\n",
    "    res_u_ = u_ - gx\n",
    "    res_v_ = v_ - hy\n",
    "    return [res_u_, res_v_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gt_index(data):\n",
    "    for k in data[\"alignment_results\"]:\n",
    "        if data[\"alignment_results\"][k][\"isGroundTruthTemplate\"] == 1:\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "def find_mpt(tem_pt, alignment_params):\n",
    "\n",
    "    H, tps_weights, template_mkpts_tps_,parallex, overlap = alignment_params[\"H\"],alignment_params[\"tps_weights\"],alignment_params[\"template_mkpts_tps_\"],alignment_params[\"parallex\"],alignment_params[\"overlap\"]\n",
    "    sam_pt = transform_template_pt(tem_pt, H, tps_weights, template_mkpts_tps_,parallex, overlap, K_smooth = 5)\n",
    "    return (tem_pt, sam_pt) # ([x1,x2],[y1,y2])\n",
    "\n",
    "def pt_scale2ori( mpts,data, k):\n",
    "    tem_scale = data[\"alignment_results\"][k][\"alignment_params\"][\"template_xy_scales\"]\n",
    "    sam_scale = data[\"alignment_results\"][k][\"alignment_params\"][\"sample_xy_scales\"]\n",
    "    _mpts = []\n",
    "    for pt in mpts:\n",
    "        pt[0][0] /= tem_scale[0]\n",
    "        pt[0][1] /= tem_scale[1]\n",
    "        pt[1][0] /= sam_scale[0]\n",
    "        pt[1][1] /= sam_scale[1]\n",
    "        _mpts.append(pt)\n",
    "    return _mpts\n",
    "\n",
    "def sam_pts_rotate(mpts, data, k, rotate):\n",
    "    h,w,_ = data[\"alignment_results\"][k][\"sample_features\"][\"sam_shape\"]\n",
    "    if rotate == 90:\n",
    "        _mpts = []\n",
    "        for pt in mpts:\n",
    "            pt[1][0],pt[1][1] = h-pt[1][1],pt[1][0]\n",
    "            _mpts.append(pt)\n",
    "    elif rotate == 180:\n",
    "        _mpts = []\n",
    "        for pt in mpts:\n",
    "            pt[1][0],pt[1][1] = w-pt[1][0],h-pt[1][1]\n",
    "            _mpts.append(pt)\n",
    "    elif rotate == 270:\n",
    "        _mpts = []\n",
    "        for pt in mpts:\n",
    "            pt[1][0],pt[1][1] = pt[1][1],w-pt[1][0]\n",
    "            _mpts.append(pt)\n",
    "    elif rotate == 0:\n",
    "        _mpts = mpts\n",
    "    return _mpts\n",
    "    \n",
    "def find_tem_pts(alignment_params,img_id,tem_pts_coord_dir,expanded=False,template_w=0):\n",
    "    with open(f\"{tem_pts_coord_dir}/{img_id}.json\",'r') as f:\n",
    "        template_pts = json.load(f)[\"template_pts\"]\n",
    "\n",
    "    ## --- HxW -> resized HxW ---\n",
    "    tem_scale = alignment_params[\"template_xy_scales\"]\n",
    "    sam_scale = alignment_params[\"sample_xy_scales\"]\n",
    "    for i in range(len(template_pts)):\n",
    "        if expanded:\n",
    "            template_pts[i][0] += (0.15/1.3*template_w)\n",
    "        template_pts[i][0] *= tem_scale[0]\n",
    "        template_pts[i][1] *= tem_scale[1]\n",
    "\n",
    "    return [find_mpt(x, alignment_params) for x in template_pts]\n",
    "    \n",
    "\n",
    "def job(js):\n",
    "    with open(js, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    im_id = data[\"id\"]\n",
    "    # find gt alignment results\n",
    "    gt_index = find_gt_index(data)\n",
    "    if gt_index == -1:\n",
    "        with open(f\"{output_after_ptmap_dir}/no_gt_id.txt\", 'a') as f:\n",
    "            f.write('\\n')\n",
    "            f.write(im_id)\n",
    "        return (im_id,None,None)\n",
    "    else:\n",
    "        alignment_params = data[\"alignment_results\"][gt_index][\"alignment_params\"]\n",
    "        template_path = data[\"alignment_results\"][gt_index][\"template_path\"]\n",
    "    # find template pts(generally 16) and scale to (480,640) and map for each pt pair\n",
    "    # template_w = data[\"alignment_results\"][gt_index][\"template_features\"][\"tem_shape\"][1]\n",
    "    template_w = 0\n",
    "    ratio = data[\"alignment_results\"][gt_index][\"matched_features\"][\"mkpts_ratio2template\"]\n",
    "    ratio_modify_by_selfmatch = data[\"alignment_results\"][gt_index][\"matched_features\"]['ratio_modify_by_image0_0']\n",
    "    mpts = find_tem_pts(alignment_params,im_id,tem_pts_coord_dir,expanded=expanded,template_w=template_w)\n",
    "\n",
    "    mpts = pt_scale2ori(mpts, data, gt_index)\n",
    "    mpts = sam_pts_rotate(mpts, data, gt_index, rotate=ROTATE)\n",
    "    template_pts = [[x[0][0], x[0][1]]for x in mpts]\n",
    "    sample_pts = [[x[1][0], x[1][1]]for x in mpts]\n",
    "    o_data = {\n",
    "        \"image_id\": im_id,\n",
    "        \"template_path\":template_path,\n",
    "        \"num_mpts\": len(template_pts),\n",
    "        \"template_pts\": template_pts,\n",
    "        \"sample_pts\": sample_pts\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_after_ptmap_dir}/results/{im_id}.json\", 'w') as f:\n",
    "        json.dump(o_data, f)\n",
    "    return (im_id,ratio,ratio_modify_by_selfmatch)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GroundTruth\n",
    "gt_anno_dir = \"/data/home/yanghanlong/data/image_registration/anno\"\n",
    "##unmatched case(like template update)\n",
    "unmatched_csv = \"/data/home/yanghanlong/data/image_registration/image_id.unmatched.csv\"\n",
    "\n",
    "##return a list of distance of each pair of pts\n",
    "##modified distance, divided by sample area\n",
    "\n",
    "def calculate_distance3(image_id):\n",
    "    pred_anno_dir = f\"{output_after_ptmap_dir}/results\"\n",
    "    pre_anno = json.load(open(Path(pred_anno_dir).joinpath(image_id+'.json'), 'r'))\n",
    "    gt_anno = json.load(open(Path(gt_anno_dir).joinpath(image_id+'.json'), 'r'))\n",
    "    pre_template_pts = pre_anno[\"template_pts\"]\n",
    "    pre_sample_pts = pre_anno[\"sample_pts\"]\n",
    "    gt_template_pts = gt_anno[\"template_pts\"]\n",
    "    gt_sample_pts = gt_anno[\"sample_pts\"]\n",
    "    ##ignore [Null, Null] in gt_sample_pts\n",
    "    ignore_pt = [None, None]\n",
    "    while ignore_pt in gt_sample_pts:\n",
    "        index = gt_sample_pts.index(ignore_pt)\n",
    "        del gt_sample_pts[index]\n",
    "        del pre_sample_pts[index]\n",
    "    ##transform to np array\n",
    "    pre_sample_pts = np.array(pre_sample_pts)\n",
    "    gt_sample_pts = np.array(gt_sample_pts)\n",
    "    ##calculate distance\n",
    "    distance = [np.linalg.norm(x - y) for x,y in zip(gt_sample_pts, pre_sample_pts)]\n",
    "    ##modify\n",
    "    im = Image.open(f\"{image_dir}/{image_id}.jpg\")\n",
    "    w, h = im.size\n",
    "    assert w != 0\n",
    "    distance = [ x / (np.sqrt(w*h)*10e-4) for x in distance]\n",
    "    return distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = glob.glob(f\"{img_align_result}/*.json\")[0]\n",
    "# print(j)\n",
    "# job(j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate ptmap and ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Total file num: 4993\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4993 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4993/4993 [00:02<00:00, 1665.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Successful file num: 4636\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "## generate ptmap\n",
    "jpath_list = glob.glob(f\"{img_align_result}/*.json\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Total file num: {len(jpath_list)}\")                \n",
    "print(\"=\"*30)\n",
    "with Pool(30) as p:\n",
    "    ratio_list = p.map(job, tqdm(jpath_list))\n",
    "ratio_list = [x for x in ratio_list if x[1] != None]\n",
    "#print results info\n",
    "suc =  glob.glob(f\"{output_after_ptmap_dir}/results/*.json\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Successful file num: {len(suc)}\")  \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 4636 cases to handle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4636 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4636/4636 [00:01<00:00, 3454.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally 4616 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## calculate distance (pred vs. GT)\n",
    "##handle all files by handle 1 file, choose calculate_distance2/calculate_distance\n",
    "#unmatched id list\n",
    "unmatched_list = pd.read_csv(unmatched_csv,header=None)[0].tolist()\n",
    "jlist = glob.glob(output_after_ptmap_dir+\"/results/*.json\")\n",
    "print(f\"Totally {len(jlist)} cases to handle.\")\n",
    "dist_list = []\n",
    "dist_dict = {}\n",
    "for jterm in tqdm(jlist):\n",
    "    image_id = Path(jterm).stem\n",
    "    #skip unmatched id\n",
    "    if image_id in unmatched_list:\n",
    "        continue\n",
    "    dist = calculate_distance3(image_id) ###CHECK\n",
    "    dist_list.append(dist)\n",
    "    dist_dict[image_id] = dist\n",
    "print(f\"finally {len(dist_list)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance of all pt pairs: 3.744\n",
      "Recall@10: 95.254%\n",
      "AR@10:50:10: 97.226%\n",
      "Average ratio: 0.650122\tAverage ratio_modify_by_selfmatch: 0.650122\n",
      "Cases(ratio < 0.2): 226\tCases(ratio_modify_by_selfmatch < 0.2): 226\n",
      "Cases(ratio < 0.1): 130\tCases(ratio_modify_by_selfmatch < 0.1): 130\n",
      "Cases (ratio < 0.2) example: [('03e7b3c1-c3f2-4800-b192-bfcfb097a7a9', 0.17410714285714285, 0.17410714285714285), ('08bdf88a-eb6a-4c43-863c-584d3077bdc9', 0.020330368487928845, 0.020330368487928845), ('0866b016-3b7f-4c96-baaf-2a2da554b16d', 0.09788359788359788, 0.09788359788359788), ('0eab6054-e056-4e61-9158-5ef194485e4e', 0.1953125, 0.1953125), ('0e5f4039-d0cf-4969-bd02-28346d627c58', 0.091796875, 0.091796875), ('0fee9d92-42ec-4d10-9ba3-132ad0fb277a', 0.033203125, 0.033203125), ('069ef367-0fd2-418e-8dd9-a8c6967f0934', 0.177734375, 0.177734375), ('0ce6f5f9-a3a7-4ba8-92e7-d9a7e0d00b11', 0.13145539906103287, 0.13145539906103287), ('023ae7de-85af-4c27-9c4f-aed35a3b1b62', 0.1038961038961039, 0.1038961038961039), ('0ca3264b-f603-4a0e-b69d-39e12ee42cf4', 0.0078125, 0.0078125)]\n"
     ]
    }
   ],
   "source": [
    "##for distance3(list)\n",
    "count = sum([len(x) for x in dist_list])\n",
    "total = sum([sum(x) for x in dist_list])\n",
    "print(f\"Average distance of all pt pairs: {total/count:.3f}\")\n",
    "recall_list = []\n",
    "for threshold in range(10, 60,10):\n",
    "    suc_num = sum([(np.array(x) < threshold).sum() for x in dist_list])\n",
    "    if threshold == 10:\n",
    "        print(f\"Recall@{threshold}: {suc_num/count:.3%}\")\n",
    "    recall_list.append(suc_num/count)\n",
    "print(f\"AR@10:50:10: {sum(recall_list)/len(recall_list):.3%}\")\n",
    "# average ratio\n",
    "pure_ratio_list = [x[1] for x in ratio_list]\n",
    "print(f\"Average ratio: {sum(pure_ratio_list)/len(pure_ratio_list):.6f}\\tAverage ratio_modify_by_selfmatch: {sum([x[2] for x in ratio_list])/len(ratio_list):.6f}\")\n",
    "print(f\"Cases(ratio < 0.2): {len([x for x in pure_ratio_list if x < 0.2])}\\tCases(ratio_modify_by_selfmatch < 0.2): {len([x for x in ratio_list if x[2] < 0.2])}\")\n",
    "print(f\"Cases(ratio < 0.1): {len([x for x in pure_ratio_list if x < 0.1])}\\tCases(ratio_modify_by_selfmatch < 0.1): {len([x for x in ratio_list if x[2] < 0.1])}\")\n",
    "print(f\"Cases (ratio < 0.2) example: {[x for x in ratio_list if x[1] < 0.2][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check aver value of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success ratio(aver dist < 10): 86.5%\n",
      "090f8be4-54c8-456b-90dc-bd6d6155a902 387.7851613626865\n",
      "0ca3fe96-1a92-4ad4-87f1-0abd548762dc 302.4453143566294\n",
      "08cf4d41-faa8-45c7-b2f0-a3cb6bf28add 226.8563450932498\n",
      "0ca3264b-f603-4a0e-b69d-39e12ee42cf4 196.1612742058414\n",
      "0a74bc93-0e37-40a1-950d-2bf72844f47b 194.80233754450484\n"
     ]
    }
   ],
   "source": [
    "aver_thres = 10\n",
    "\n",
    "dist_aver_list = [sum(x)/len(x) for x in dist_list]\n",
    "print(f\"Success ratio(aver dist < {aver_thres}): {len([x for x in dist_aver_list if x < aver_thres])/len(jpath_list):.1%}\")\n",
    "dist_aver_dict = {k:(sum(v)/len(v)) for k,v in dist_dict.items()}\n",
    "max_key = sorted(dist_aver_dict.keys(), key=dist_aver_dict.get,reverse=True)\n",
    "for i in range(5):\n",
    "    print(f\"{max_key[i]} {dist_aver_dict[max_key[i]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check max value of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01a332e3-445b-4e64-8d84-e5e19c9ac8b2 8304.688879009067\n",
      "0e14e5d6-a38e-4efc-87b0-88d7ff682dec 7980.670276769891\n",
      "0c086640-689c-4070-acdf-07019455e1b5 4372.753874706445\n",
      "0f3ce993-cd5b-4fd5-affd-133d16996f9b 3993.8934128345895\n",
      "0fb387db-c1bc-451a-8a57-d7a001065d67 2145.3819529456996\n"
     ]
    }
   ],
   "source": [
    "dist_sum_dict = {k:sum(v) for k,v in dist_dict.items()}\n",
    "max_key = sorted(dist_sum_dict.keys(), key=dist_sum_dict.get,reverse=True)\n",
    "for i in range(5):\n",
    "    print(f\"{max_key[i]} {dist_sum_dict[max_key[i]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p = \"/data/home/yanghanlong/mymodel_v0811_2048_img_align_ptmap5k_results.csv\"\n",
    "\n",
    "\n",
    "dist_sum_list = [dist_sum_dict[x] for x in max_key]\n",
    "o = pd.DataFrame({'image_id':max_key,'distance_sum':dist_sum_list})\n",
    "o.to_csv(results_p,index=False,sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy [blended, sample, matches] in bad case to one folder **(need run max_key first)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_chech_dir = Path(\"/data/home/yanghanlong/temp/v3_5_6_epoch14\")\n",
    "copy_num = 20\n",
    "\n",
    "dist_sum_dict = {k:sum(v) for k,v in dist_dict.items()}\n",
    "max_key = sorted(dist_sum_dict.keys(), key=dist_sum_dict.get,reverse=True)\n",
    "def copyImgAlignResults(image_id_list,output_dir,image_align_results_dir=None):\n",
    "    if image_align_results_dir == None:\n",
    "        image_align_results_dir = img_align_result\n",
    "    for image_id in image_id_list:\n",
    "        shutil.copytree(f\"{img_align_result}/{image_id}\",Path(output_dir) / image_id)\n",
    "\n",
    "# temp_chech_dir = Path(\"/data/home/yanghanlong/temp/original_model\")\n",
    "temp_chech_dir.mkdir(exist_ok=True)\n",
    "copyImgAlignResults(max_key[0:copy_num],temp_chech_dir)\n",
    "# copyImgAlignResults(aa,temp_chech_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>distance_sum_nodel</th>\n",
       "      <th>distance_sum_del</th>\n",
       "      <th>diff[(_nodel)-(_del)]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>022d022b-e9d0-4132-8188-769a311784a0</td>\n",
       "      <td>5.177852</td>\n",
       "      <td>342.397757</td>\n",
       "      <td>-337.219905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0163d0ff-f079-4267-bd12-146e3cfd6bc0</td>\n",
       "      <td>9.673700</td>\n",
       "      <td>116.359219</td>\n",
       "      <td>-106.685519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0e71b233-7d73-45c9-bec4-2959bd7394f7</td>\n",
       "      <td>17.705952</td>\n",
       "      <td>121.882608</td>\n",
       "      <td>-104.176656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0adbe9f9-9bce-4988-9954-761136a5e32a</td>\n",
       "      <td>9.641336</td>\n",
       "      <td>22.222624</td>\n",
       "      <td>-12.581288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0cfd205a-c30f-4dba-9088-ab56e22d7ea2</td>\n",
       "      <td>4.688163</td>\n",
       "      <td>15.448004</td>\n",
       "      <td>-10.759842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0b48ea64-6d3f-453c-ae7f-2f9003988201</td>\n",
       "      <td>15.411926</td>\n",
       "      <td>11.663885</td>\n",
       "      <td>3.748041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0087e504-8558-4144-88c5-9b24ba28dc1d</td>\n",
       "      <td>49.909473</td>\n",
       "      <td>45.200406</td>\n",
       "      <td>4.709067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02fd9ec8-f8b1-44b1-b295-e4421867de04</td>\n",
       "      <td>61.058938</td>\n",
       "      <td>55.022899</td>\n",
       "      <td>6.036038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>098924c3-6dbc-4de6-878f-1ba7c410f947</td>\n",
       "      <td>12.386085</td>\n",
       "      <td>6.252266</td>\n",
       "      <td>6.133819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ebb6d30-666e-49de-b529-f72c0a23b37c</td>\n",
       "      <td>153.362582</td>\n",
       "      <td>15.490056</td>\n",
       "      <td>137.872526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                image_id  distance_sum_nodel   \n",
       "0   022d022b-e9d0-4132-8188-769a311784a0            5.177852  \\\n",
       "2   0163d0ff-f079-4267-bd12-146e3cfd6bc0            9.673700   \n",
       "3   0e71b233-7d73-45c9-bec4-2959bd7394f7           17.705952   \n",
       "4   0adbe9f9-9bce-4988-9954-761136a5e32a            9.641336   \n",
       "5   0cfd205a-c30f-4dba-9088-ab56e22d7ea2            4.688163   \n",
       "..                                   ...                 ...   \n",
       "10  0b48ea64-6d3f-453c-ae7f-2f9003988201           15.411926   \n",
       "9   0087e504-8558-4144-88c5-9b24ba28dc1d           49.909473   \n",
       "7   02fd9ec8-f8b1-44b1-b295-e4421867de04           61.058938   \n",
       "6   098924c3-6dbc-4de6-878f-1ba7c410f947           12.386085   \n",
       "1   0ebb6d30-666e-49de-b529-f72c0a23b37c          153.362582   \n",
       "\n",
       "    distance_sum_del  diff[(_nodel)-(_del)]  \n",
       "0         342.397757            -337.219905  \n",
       "2         116.359219            -106.685519  \n",
       "3         121.882608            -104.176656  \n",
       "4          22.222624             -12.581288  \n",
       "5          15.448004             -10.759842  \n",
       "..               ...                    ...  \n",
       "10         11.663885               3.748041  \n",
       "9          45.200406               4.709067  \n",
       "7          55.022899               6.036038  \n",
       "6           6.252266               6.133819  \n",
       "1          15.490056             137.872526  \n",
       "\n",
       "[4972 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = pd.read_csv('/data/home/yanghanlong/compare.csv',sep='\\t')\n",
    "# 排序dt[\"diff[(_nodel)-(_del)]\"]列中负数中绝对值最大的20个image_id\n",
    "dt = dt.sort_values(by=\"diff[(_nodel)-(_del)]\",ascending=True)\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare two csv results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1=\"/data/home/yanghanlong/mymodel_v0811_2048_img_align_ptmap5k_results.csv\"\n",
    "csv2=\"/data/home/yanghanlong/mymodel_v0811_2048_del_img_align_ptmap5k_results.csv\"\n",
    "compare_output = \"/data/home/yanghanlong/compare.csv\"\n",
    "suffixes = ('_nodel','_del')\n",
    "\n",
    "df1 = pd.read_csv(csv1,sep='\\t')\n",
    "df2 = pd.read_csv(csv2,sep='\\t')\n",
    "# merge and diff\n",
    "merge=pd.merge(df1,df2,on='image_id',suffixes=suffixes)\n",
    "diff_col_name = f'diff[({suffixes[0]})-({suffixes[1]})]'\n",
    "merge[diff_col_name] = merge[f\"distance_sum{suffixes[0]}\"] - merge[f\"distance_sum{suffixes[1]}\"]\n",
    "# sort\n",
    "sorted_merge = merge.sort_values(by=diff_col_name, key=lambda x: abs(x), ascending=False)\n",
    "sorted_merge.to_csv(compare_output,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- positive means better, negative means worse\n",
      "diff postive num: 284\n",
      "diff negative num: 299\n",
      "diff > 10 num: 1\n",
      "diff < 10 num: 5\n"
     ]
    }
   ],
   "source": [
    "print('---- positive means better, negative means worse')\n",
    "print(f\"diff postive num: {(sorted_merge[diff_col_name] > 0).sum()}\")\n",
    "print(f\"diff negative num: {(sorted_merge[diff_col_name] < 0).sum()}\")\n",
    "print(f\"diff > 10 num: {(sorted_merge[diff_col_name] > 10).sum()}\")\n",
    "print(f\"diff < 10 num: {(sorted_merge[diff_col_name] < -10).sum()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
